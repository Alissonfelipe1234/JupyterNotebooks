{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk.corpus\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a software that uses LDA model from nltk library and creates a metric \n",
    "\n",
    "\n",
    "- How to run on Linux:\n",
    "    - sudo pip install -U numpy\n",
    "    - sudo pip install -U gensim\n",
    "    - sudo pip install -U openpyxl\n",
    "    - sudo pip install -U nltk\n",
    "    - cd /usr/local/lib/python3.5/dist-packages\n",
    "    - sudo python -m nltk.downloader stopwords\n",
    "    - sudo python -m nltk.downloader wordnet\n",
    "    \n",
    "- Windows:\n",
    "    - pip install -U nltk\n",
    "    - pip install -U numpy\n",
    "    - pip install -U gensim\n",
    "    - pip install -U openpyxl\n",
    "    - python\n",
    "    <br>\n",
    "    <font color=red>\n",
    "    import nltk\n",
    "    <br>\n",
    "    nltk.download('stopwords')\n",
    "    <br>\n",
    "    nltk.download('wordnet')\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arion:\n",
    "    @staticmethod\n",
    "    def mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    @staticmethod\n",
    "    def weighted_mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs(((y_true*y_pred) - (y_pred*y_pred)) / (y_true* y_pred))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Research:\n",
    "    @staticmethod\n",
    "    def parse_xls(xls_file=None):\n",
    "        if not xls_file:\n",
    "            return False\n",
    "        df = pd.read_excel(xls_file)\n",
    "        return list(df['Abstract Note'])\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_scholar(query_word='security', size=6, initial_time=None, end_time=None):\n",
    "        import scholarly\n",
    "        q = '/scholar?lr=lang_us&q='+query_word+'&hl=en-US&as_vis=1&as_sdt=1,5'\n",
    "        if(initial_time != None):\n",
    "            q = q + '&as_ylo=' + initial_time\n",
    "        if(end_time != None):\n",
    "            q = q +'&as_yhi=' + end_time\n",
    "        searchFilter = scholarly.search_pubs_custom_url(q)\n",
    "        return [next(searchFilter).bib['abstract'] for i in range(size)]\n",
    "    @staticmethod\n",
    "    def words_stop(stopPath):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        with open(stopPath, \"rb\") as msw:\n",
    "            my_stops = msw.read().decode('utf-8').split(\"\\r\\n\")            \n",
    "            stop_words.extend(my_stops)\n",
    "        return stop_words\n",
    "    @staticmethod    \n",
    "    def clean(doc, stopPath):\n",
    "        exclude = set(string.punctuation)\n",
    "        lemma = WordNetLemmatizer()          \n",
    "        stop = set(stopwords.words('english'))      \n",
    "        my_stops = Research.words_stop(stopPath)\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        return \" \".join([i for i in normalized.split() if i not in my_stops])\n",
    "    @staticmethod\n",
    "    def save(path, doc_list):\n",
    "        with open(path, \"wb\") as file:\n",
    "            doc_str = str(doc_list).encode(\"utf-8\")\n",
    "            file.write(doc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleaner:\n",
    "    def __init__(self):\n",
    "        self.result = []\n",
    "    @staticmethod\n",
    "    def clean_xls(xls_file_in=None, xls_file_out=None):\n",
    "        if not xls_file_in:\n",
    "            return False\n",
    "        if not xls_file_out:\n",
    "            xls_file_out = xls_file_in\n",
    "\n",
    "        data = pd.read_excel(xls_file_in, index_col=0)\n",
    "        data = clean_panda(data)\n",
    "        data.to_excel(xls_file_out)\n",
    "        \n",
    "    @staticmethod\n",
    "    def clean_csv(csv_file_in=None, csv_file_out=None):\n",
    "        if not csv_file_in:\n",
    "            return False\n",
    "        if not csv_file_out:\n",
    "            csv_file_out = csv_file_in\n",
    "\n",
    "        data = pd.read_csv(csv_file_in, index_col=0)\n",
    "        data = clean_panda(data)\n",
    "        data.to_csv(csv_file_out)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_panda(data):\n",
    "        data[\"abstract\"] = data[\"abstract\"].apply(lambda x: re.sub(\"([©]*)\\.\",\"\",x))#remove copyright's\n",
    "        data[\"abstract\"] = data[\"abstract\"].apply(lambda x: re.sub('\\S*@\\S*\\s?', '', x))#remove emails\n",
    "        data[\"abstract\"] = data[\"abstract\"].apply(lambda x: re.sub('\\s+', ' ', x))#\n",
    "        data[\"abstract\"] = data[\"abstract\"].apply(lambda x: re.sub(\"\\'\", ' ', x))# remove '\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set variables\n",
    "source = 'Papers.xlsx'\n",
    "stopWords = 'BlockWords.txt'\n",
    "clean_path = 'CleanArchive.txt'\n",
    "log_path = 'BeforeArchive.txt'\n",
    "topics = 10\n",
    "words = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare docs\n",
    "#log = research.revoke(source)\n",
    "doc_complete = Research.parse_xls(source)\n",
    "doc_clean = [Research.clean(doc, stopWords).split() for doc in doc_complete]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this version and alterations\n",
    "Research.save(log_path, []) #log)\n",
    "Research.save(clean_path, doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary and matrix\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda model\n",
    "LDA = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA result\n",
    "result = LDA(doc_term_matrix, num_topics=topics, eval_every=10, chunksize=words, id2word=dictionary, passes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus ({iterável da lista de (int, float), SciPy. SPARSE. CSC}, opcional) – fluxo de vetores de documentos ou matriz esparsa de forma (num_terms, num_documents). Se não for fornecido, o modelo é deixado não treinado (presumivelmente porque você deseja chamar Update () manualmente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim.models.ldamodel.LdaModel\n",
    "(corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf=None, minimum_phi_value=0.01, per_word_topics=False, callbacks=None, dtype=<type 'numpy.float32'>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = result.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = [0.01 for x in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008497698, 0.007894334, 0.007667139, 0.0076317564, 0.006856583, 0.006652674]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = []\n",
    "for x in range(10):\n",
    "    for y in range(10):\n",
    "        predict.append(table[x][1][y][1])\n",
    "predict[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a65a3c904616>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Do lemmatization keeping only noun, adj, vb, adv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_lemmatized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words_bigrams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_words_bigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_bigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words_nostops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatization' is not defined"
     ]
    }
   ],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', result.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=result, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.07335047610104\n",
      "36.073350239443144\n"
     ]
    }
   ],
   "source": [
    "print(Arion.mean_absolute_percentage_error(real,predict))\n",
    "print(Arion.weighted_mean_absolute_percentage_error(real,predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
