{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "#from Research import Research\n",
    "import csv\n",
    "import datetime\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import string\n",
    "from openpyxl import load_workbook\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import nltk.corpus\n",
    "#import Seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a software that uses LDA model from nltk library and creates a metric \n",
    "\n",
    "\n",
    "- How to run on Linux:\n",
    "    - sudo pip install -U numpy\n",
    "    - sudo pip install -U gensim\n",
    "    - sudo pip install -U openpyxl\n",
    "    - sudo pip install -U nltk\n",
    "    - cd /usr/local/lib/python3.5/dist-packages\n",
    "    - sudo python -m nltk.downloader stopwords\n",
    "    - sudo python -m nltk.downloader wordnet\n",
    "    \n",
    "- Windows:\n",
    "    - pip install -U nltk\n",
    "    - pip install -U numpy\n",
    "    - pip install -U gensim\n",
    "    - pip install -U openpyxl\n",
    "    - python\n",
    "    <br>\n",
    "    <font color=red>\n",
    "    import nltk\n",
    "    <br>\n",
    "    nltk.download('stopwords')\n",
    "    <br>\n",
    "    nltk.download('wordnet')\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arion:\n",
    "    @staticmethod\n",
    "    def mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    @staticmethod\n",
    "    def weighted_mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs(((y_true*y_pred) - (y_pred*y_pred)) / (y_true* y_pred))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Research:\n",
    "    @staticmethod\n",
    "    def parse_xls(xls_file=None):\n",
    "        if not xls_file:\n",
    "            return False\n",
    "        df = pd.read_excel(xls_file)\n",
    "        return list(df['Abstract Note'])\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_scholar(query_word='security', size=6, initial_time=None, end_time=None):\n",
    "        import scholarly\n",
    "        q = '/scholar?lr=lang_us&q='+query_word+'&hl=en-US&as_vis=1&as_sdt=1,5'\n",
    "        if(initial_time != None):\n",
    "            q = q + '&as_ylo=' + initial_time\n",
    "        if(end_time != None):\n",
    "            q = q +'&as_yhi=' + end_time\n",
    "        searchFilter = scholarly.search_pubs_custom_url(q)\n",
    "        return [next(searchFilter).bib['abstract'] for i in range(size)]\n",
    "    @staticmethod\n",
    "    def words_stop(stopPath):\n",
    "        with open(stopWords, \"rb\") as msw:\n",
    "            meuStopWord = msw.read().decode('utf-8').split(\"\\r\\n\")\n",
    "        return set(stopwords.words('english'))\n",
    "    @staticmethod    \n",
    "    def clean(doc, stopWords):\n",
    "        exclude = set(string.punctuation)\n",
    "        lemma = WordNetLemmatizer()        \n",
    "        stop = Research.words_stop(stopWords)\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        return \" \".join([i for i in normalized.split() if i not in meuStopWord])\n",
    "    @staticmethod\n",
    "    def save(path, doc_list):\n",
    "        with open(path, \"wb\") as file:\n",
    "            doc_str = str(doc_list).encode(\"utf-8\")\n",
    "            file.write(doc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleaner:\n",
    "    def __init__(self):\n",
    "        self.result = []\n",
    "    @staticmethod\n",
    "    def clean_xls(xls_file_in=None, xls_file_out=None):\n",
    "        if not xls_file_in:\n",
    "            return False\n",
    "        if not xls_file_out:\n",
    "            xls_file_out = xls_file_in\n",
    "\n",
    "        data = pd.read_excel(xls_file_in, index_col=0)\n",
    "        data = clean_panda(data)\n",
    "        data.to_excel(xls_file_out)\n",
    "        \n",
    "    @staticmethod\n",
    "    def clean_csv(csv_file_in=None, csv_file_out=None):\n",
    "        if not csv_file_in:\n",
    "            return False\n",
    "        if not csv_file_out:\n",
    "            csv_file_out = csv_file_in\n",
    "\n",
    "        data = pd.read_csv(csv_file_in, index_col=0)\n",
    "        data = clean_panda(data)\n",
    "        data.to_csv(csv_file_out)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_panda(data):\n",
    "        data[\"abstract\"] = data[\"abstract\"].apply(lambda x: re.sub(\"([©]*)\\.\",\"\",x))#remove copyright's\n",
    "        data[\"abstract\"] = data[\"abstract\"].apply(lambda x: re.sub('\\S*@\\S*\\s?', '', x))#remove emails\n",
    "        data[\"abstract\"] = data[\"abstract\"].apply(lambda x: re.sub('\\s+', ' ', x))#\n",
    "        data[\"abstract\"] = data[\"abstract\"].apply(lambda x: re.sub(\"\\'\", ' ', x))# remove '\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set variables\n",
    "source = 'Papers.xlsx'\n",
    "stopWords = 'BlockWords.txt'\n",
    "clean_path = 'CleanArchive.txt'\n",
    "log_path = 'BeforeArchive.txt'\n",
    "topics = 10\n",
    "words = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare docs\n",
    "#log = research.revoke(source)\n",
    "doc_complete = Research.parse_xls(source)\n",
    "doc_clean = [Research.clean(doc, stopWords).split() for doc in doc_complete]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this version and alterations\n",
    "Research.save(log_path, []) #log)\n",
    "Research.save(clean_path, doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary and matrix\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda model\n",
    "LDA = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA result\n",
    "result = LDA(doc_term_matrix, num_topics=topics, id2word=dictionary, passes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus ({iterável da lista de (int, float), SciPy. SPARSE. CSC}, opcional) – fluxo de vetores de documentos ou matriz esparsa de forma (num_terms, num_documents). Se não for fornecido, o modelo é deixado não treinado (presumivelmente porque você deseja chamar Update () manualmente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim.models.ldamodel.LdaModel\n",
    "(corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf=None, minimum_phi_value=0.01, per_word_topics=False, callbacks=None, dtype=<type 'numpy.float32'>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = result.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = [0.01 for x in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008497698, 0.007894334, 0.007667139, 0.0076317564, 0.006856583, 0.006652674]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = []\n",
    "for x in range(10):\n",
    "    for y in range(10):\n",
    "        predict.append(table[x][1][y][1])\n",
    "predict[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.07335047610104\n",
      "36.073350239443144\n"
     ]
    }
   ],
   "source": [
    "print(Arion.mean_absolute_percentage_error(real,predict))\n",
    "print(Arion.weighted_mean_absolute_percentage_error(real,predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
