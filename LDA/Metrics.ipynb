{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk.corpus\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a software that uses LDA model from nltk library and creates a metric \n",
    "\n",
    "\n",
    "- How to run on Linux:\n",
    "    - sudo pip install -U numpy\n",
    "    - sudo pip install -U gensim\n",
    "    - sudo pip install -U openpyxl\n",
    "    - sudo pip install -U nltk\n",
    "    - cd /usr/local/lib/python3.5/dist-packages\n",
    "    - sudo python -m nltk.downloader stopwords\n",
    "    - sudo python -m nltk.downloader wordnet\n",
    "    \n",
    "- Windows:\n",
    "    - pip install -U nltk\n",
    "    - pip install -U numpy\n",
    "    - pip install -U gensim\n",
    "    - pip install -U openpyxl\n",
    "    - python\n",
    "    <br>\n",
    "    <font color=red>\n",
    "    import nltk\n",
    "    <br>\n",
    "    nltk.download('stopwords')\n",
    "    <br>\n",
    "    nltk.download('wordnet')\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    @staticmethod\n",
    "    def mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    @staticmethod\n",
    "    def weighted_mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs(((y_true*y_pred) - (y_pred*y_pred)) / (y_true* y_pred))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Research:\n",
    "    @staticmethod\n",
    "    def parse_xls(xls_file=None):\n",
    "        if not xls_file:\n",
    "            return False\n",
    "        df = pd.read_excel(xls_file)\n",
    "        return list(df['Abstract Note'])\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_scholar(query_word='security', size=6, initial_time=None, end_time=None):\n",
    "        import scholarly\n",
    "        q = '/scholar?lr=lang_us&q='+query_word+'&hl=en-US&as_vis=1&as_sdt=1,5'\n",
    "        if(initial_time != None):\n",
    "            q = q + '&as_ylo=' + initial_time\n",
    "        if(end_time != None):\n",
    "            q = q +'&as_yhi=' + end_time\n",
    "        searchFilter = scholarly.search_pubs_custom_url(q)\n",
    "        return [next(searchFilter).bib['abstract'] for i in range(size)]\n",
    "    @staticmethod\n",
    "    def words_stop(stopPath):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        with open(stopPath, \"rb\") as msw:\n",
    "            my_stops = msw.read().decode('utf-8').split(\"\\r\\n\")            \n",
    "            stop_words.extend(my_stops)\n",
    "        return stop_words\n",
    "    @staticmethod    \n",
    "    def clean(doc, stopPath):\n",
    "        exclude = set(string.punctuation)\n",
    "        lemma = WordNetLemmatizer()          \n",
    "        stop = set(stopwords.words('english'))      \n",
    "        my_stops = Research.words_stop(stopPath)\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        return \" \".join([i for i in normalized.split() if i not in my_stops])\n",
    "    @staticmethod\n",
    "    def save(path, doc_list):\n",
    "        with open(path, \"wb\") as file:\n",
    "            doc_str = str(doc_list).encode(\"utf-8\")\n",
    "            file.write(doc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleaner:\n",
    "    def __init__(self):\n",
    "        self.result = []\n",
    "    @staticmethod\n",
    "    def clean_xls(xls_file_in=None, xls_file_out=None):\n",
    "        if not xls_file_in:\n",
    "            return False\n",
    "        if not xls_file_out:\n",
    "            xls_file_out = xls_file_in\n",
    "\n",
    "        data = pd.read_excel(xls_file_in, index_col=0)\n",
    "        data = clean_panda(data)\n",
    "        data.to_excel(xls_file_out)\n",
    "        \n",
    "    @staticmethod\n",
    "    def clean_csv(csv_file_in=None, csv_file_out=None):\n",
    "        if not csv_file_in:\n",
    "            return False\n",
    "        if not csv_file_out:\n",
    "            csv_file_out = csv_file_in\n",
    "\n",
    "        data = pd.read_csv(csv_file_in, index_col=0)\n",
    "        data = clean_panda(data)\n",
    "        data.to_csv(csv_file_out)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_panda(data):\n",
    "        data[\"abstract\"] = data[\"abstract\"].apply(lambda x: re.sub(\"([©]*)\\.\",\"\",x))#remove copyright's\n",
    "        data[\"abstract\"] = data[\"abstract\"].apply(lambda x: re.sub('\\S*@\\S*\\s?', '', x))#remove emails\n",
    "        data[\"abstract\"] = data[\"abstract\"].apply(lambda x: re.sub('\\s+', ' ', x))#\n",
    "        data[\"abstract\"] = data[\"abstract\"].apply(lambda x: re.sub(\"\\'\", ' ', x))# remove '\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set variables\n",
    "source = 'Papers.xlsx'\n",
    "stopWords = 'BlockWords.txt'\n",
    "clean_path = 'CleanArchive.txt'\n",
    "log_path = 'BeforeArchive.txt'\n",
    "topics = 10\n",
    "words = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare docs\n",
    "#log = research.revoke(source)\n",
    "doc_complete = Research.parse_xls(source)\n",
    "doc_clean = [Research.clean(doc, stopWords).split() for doc in doc_complete]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this version and alterations\n",
    "Research.save(log_path, []) #log)\n",
    "Research.save(clean_path, doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary and matrix\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda model\n",
    "LDA = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA result\n",
    "result = LDA(doc_term_matrix, num_topics=topics, eval_every=10, chunksize=words, id2word=dictionary, passes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Na prática (Corpus =/= Corpus de formação inicial), mas usamos o mesmo aqui para simplificar.\n",
    "#O modelo pode ser atualizado (treinado) com novos documentos.\n",
    "other_corpus = common_corpus\n",
    "\n",
    "lda.update(other_corpus)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> from gensim.corpora import Dictionary\n",
    ">>>\n",
    ">>> corpus = [[\"a\", \"a\", \"b\"], [\"a\", \"c\"]]\n",
    ">>> dct = Dictionary(corpus)\n",
    ">>> dct.doc2idx([\"a\", \"a\", \"c\", \"not_in_dictionary\", \"c\"])\n",
    "[0, 0, 2, -1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A persistência do modelo é obtida através dos métodos load () e save ().\n",
    "\n",
    "## Parâmetros:\n",
    "##### corpus ({iterável da lista de (int, float), scipy.sparse.csc}, opcional) \n",
    "- Fluxo de vetores de documento ou matriz esparsa de forma (num_terms, num_documents). Se não for fornecido, o modelo será deixado sem treinamento (presumivelmente porque você deseja chamar update () manualmente).\n",
    "\n",
    "##### num_topics (int, optional) \n",
    "- O número de tópicos latentes solicitados a serem extraídos do corpus de treinamento.\n",
    "\n",
    "#####  id2word ({dict of (int, str), gensim.corpora.dictionary.Dictionary}) \n",
    "- Mapeamento de IDs de palavras em palavras. Ele é usado para determinar o tamanho do vocabulário, bem como para depuração e impressão de tópicos.\n",
    "\n",
    "#####  distributed (bool, optional) \n",
    "- Se a computação distribuída deve ser usada para acelerar o treinamento.\n",
    "\n",
    "##### chunksize (int, optional) \n",
    "- Número de documentos a serem usados ​​em cada bloco de treinamento.\n",
    "\n",
    "#####  passes (int, opcional) \n",
    "- Número de passagens pelo corpus durante o treinamento.\n",
    "\n",
    "#####  update_every (int, opcional) \n",
    "- Número de documentos a serem repetidos para cada atualização. Definido como 0 para aprendizado em lote,> 1 para aprendizado interativo iterativo.\n",
    "\n",
    "#####  alpha ({numpy.ndarray, str}, opcional) \n",
    "- Pode ser definido para um array 1D de comprimento igual ao número de tópicos esperados que expressam nossa crença a priori para a probabilidade de cada tópico. Alternativamente, as estratégias de seleção prévia padrão podem ser empregadas fornecendo uma string:\n",
    "\n",
    ">  'Assimétrico': Usa um prior assimétrico fixo normalizado de 1.0 / topicno.\n",
    "\n",
    ">  'Auto': aprende um prior assimétrico do corpus (não disponível se distribuído == True).\n",
    "\n",
    "#####  eta ({float, np.array, str}, opcional) - A priori crença na probabilidade da palavra, isso pode ser:\n",
    "- escalar para um prior simétrico sobre probabilidade de tópico / palavra, vetor de comprimento num_words para denotar uma probabilidade definida pelo usuário assimétrica para cada palavra, matriz de forma (num_topics, num_words) para atribuir uma probabilidade para cada combinação de tópicos de palavras,  a string \"auto\" para aprender a priori assimétrica a partir dos dados.\n",
    "\n",
    "##### decay (float, optional) \n",
    "- Um número entre (0.5, 1) para pesar que porcentagem do valor lambda anterior é esquecida quando cada novo documento é examinado Corresponde ao Kappa de Matthew D. Hoffman, David M. Blei e Francis Bach: “Aprendizagem on-line para a alocação de Dirichlet Latent NIPS'10”.\n",
    "\n",
    "##### offset (flutuante, opcional)\n",
    "- Hyper-parâmetro que controla o quanto vamos desacelerar os primeiros passos nas primeiras iterações. Corresponde a Tau_0 de Matthew D. Hoffman, David M. Blei e Francis Bach: “Aprendizagem on-line para alocação de Dirichlet latente NIPS '10”.\n",
    "\n",
    "##### eval_every (int, opcional) \n",
    "- A perplexidade do log é estimada em cada uma das muitas atualizações. Definir isso para um diminui o treinamento em ~ 2x.\n",
    "\n",
    "##### iterações (int, opcional) \n",
    "- Número máximo de iterações no corpus ao inferir a distribuição de tópico de um corpus.\n",
    "\n",
    "##### gamma_threshold (float, optional) \n",
    "- Mínima alteração no valor dos parâmetros gama para continuar a iteração.\n",
    "\n",
    "##### minimum_probability (float, opcional) \n",
    "- Tópicos com uma probabilidade menor que esse limite serão filtrados.\n",
    "\n",
    "##### random_state ({np.random.RandomState, int}, opcional)\n",
    "- Um objeto randomState ou um seed para gerar um. Útil para reprodutibilidade.\n",
    "\n",
    "##### ns_conf (dict de (str, objeto), opcional)\n",
    "- Parâmetros de palavra chave propagados para gensim.utils.getNS () para obter um Pyro4 Nameserved. Usado somente se distribuído estiver definido como True.\n",
    "\n",
    "##### minimum_phi_value (float, opcional) \n",
    "- se per_word_topics for True, isso representa um limite inferior no termo probabilidades.\n",
    "\n",
    "##### per_word_topics (bool)\n",
    "- Se for True, o modelo também calcula uma lista de tópicos, classificados em ordem decrescente dos tópicos mais prováveis para cada palavra, juntamente com seus valores phi multiplicados pelo comprimento do recurso (ou seja, contagem de palavras).\n",
    "\n",
    "##### callbacks (lista de retorno de chamada) \n",
    "- Retornos métricos para registrar e visualizar as métricas de avaliação do modelo durante o treinamento.\n",
    "\n",
    "##### dtype ({numpy.float16, numpy.float32, numpy.float64}, opcional) \n",
    "- Tipo de dados a ser usado durante os cálculos dentro do modelo. Todas as entradas também são convertidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim.models.ldamodel.LdaModel\n",
    "(corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf=None, minimum_phi_value=0.01, per_word_topics=False, callbacks=None, dtype=<type 'numpy.float32'>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = result.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = [0.01 for x in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008497698, 0.007894334, 0.007667139, 0.0076317564, 0.006856583, 0.006652674]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = []\n",
    "for x in range(10):\n",
    "    for y in range(10):\n",
    "        predict.append(table[x][1][y][1])\n",
    "predict[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a65a3c904616>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Do lemmatization keeping only noun, adj, vb, adv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_lemmatized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words_bigrams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_words_bigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_bigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words_nostops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatization' is not defined"
     ]
    }
   ],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', result.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=result, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.07335047610104\n",
      "36.073350239443144\n"
     ]
    }
   ],
   "source": [
    "print(Metrics.mean_absolute_percentage_error(real,predict))\n",
    "print(Metrics.weighted_mean_absolute_percentage_error(real,predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
